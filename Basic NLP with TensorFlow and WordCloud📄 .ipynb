{"cells":[{"metadata":{},"cell_type":"markdown","source":"<a id='top'></a>\n<h1 style=\"text-align:center;font-size:200%;;\">Real or Not? NLP with Disaster Tweets</h1>\n<img src=\"https://dataxboost.files.wordpress.com/2018/03/nlp.jpg\">"},{"metadata":{},"cell_type":"markdown","source":"## Competition Description\n* Twitter has become an important communication channel in times of emergency.\nThe ubiquitousness of smartphones enables people to announce an emergency theyâ€™re observing in real-time. Because of this, more agencies are interested in programatically monitoring Twitter (i.e. disaster relief organizations and news agencies)."},{"metadata":{},"cell_type":"markdown","source":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n  <h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\">Notebook Content!</h3>\n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#libraries\" role=\"tab\" aria-controls=\"profile\">Import Libraries<span class=\"badge badge-primary badge-pill\">1</span></a>\n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#load\" role=\"tab\" aria-controls=\"messages\">Load Data<span class=\"badge badge-primary badge-pill\">2</span></a>\n  <a class=\"list-group-item list-group-item-action\"  data-toggle=\"list\" href=\"#visual\" role=\"tab\" aria-controls=\"settings\">Visualization of data<span class=\"badge badge-primary badge-pill\">3</span></a>\n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#word\" role=\"tab\" aria-controls=\"settings\">WordCloud<span class=\"badge badge-primary badge-pill\">4</span></a> \n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#clean\" role=\"tab\" aria-controls=\"settings\">Cleaning the text<span class=\"badge badge-primary badge-pill\">5</span></a>\n    <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#split\" role=\"tab\" aria-controls=\"settings\">Train and test Split<span class=\"badge badge-primary badge-pill\">6</span></a>\n    <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#model\" role=\"tab\" aria-controls=\"settings\"> Creating the Model<span class=\"badge badge-primary badge-pill\">7</span></a>\n    <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#eval\" role=\"tab\" aria-controls=\"settings\">Model Evaluation<span class=\"badge badge-primary badge-pill\">8</span></a>  "},{"metadata":{},"cell_type":"markdown","source":"<a id='libraries'></a>\n## 1. Import Libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport tensorflow as tf\nimport time \nfrom wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\nfrom IPython.core.display import display, HTML\nimport plotly.graph_objects as go\nimport re\n# Natural Language Tool Kit \nimport nltk  \nnltk.download('stopwords') \nfrom nltk.corpus import stopwords \nfrom nltk.stem.porter import PorterStemmer \nfrom collections import Counter\nimport cufflinks as cf\ncf.go_offline()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='load'></a>\n# 2. Load Data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\nsubmission =  pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<ul style=\"list-style-type:square;\">\n  <li><span class=\"label label-default\">id</span> a unique identifier for each tweet</li>\n  <li><span class=\"label label-default\">text </span> the text of the tweet</li>\n  <li><span class=\"label label-default\">location</span>  the location the tweet was sent from (may be blank)</li>\n    <li><span class=\"label label-default\">keyword</span>  a particular keyword from the tweet (may be blank)</li>\n    <li><span class=\"label label-default\">target</span>  in train.csv only, this denotes whether a tweet is about a real disaster (1) or not (0)</li>\n</ul>\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"display(HTML(f\"\"\"\n   \n        <ul class=\"list-group\">\n          <li class=\"list-group-item disabled\" aria-disabled=\"true\"><h4>Shape of Train and Test Dataset</h4></li>\n          <li class=\"list-group-item\"><h4>Number of rows in Train dataset is: <span class=\"label label-primary\">{ train.shape[0]:,}</span></h4></li>\n          <li class=\"list-group-item\"> <h4>Number of columns Train dataset is <span class=\"label label-primary\">{train.shape[1]}</span></h4></li>\n          <li class=\"list-group-item\"><h4>Number of rows in Test dataset is: <span class=\"label label-success\">{ test.shape[0]:,}</span></h4></li>\n          <li class=\"list-group-item\"><h4>Number of columns Test dataset is <span class=\"label label-success\">{test.shape[1]}</span></h4></li>\n        </ul>\n  \n    \"\"\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='visual'></a>\n# 3. Visualization of data"},{"metadata":{"trusted":true},"cell_type":"code","source":"missing = train.isnull().sum()  \nmissing[missing>0].sort_values(ascending=False).iplot(kind='bar',title='Null values present in train Dataset', color=['red'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.target.value_counts().iplot(kind='bar',text=['Fake', 'Real'], title='Comparing Tweet is a real disaster (1) or not (0)',color=['blue'])","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"counts_train = train.target.value_counts(sort=False)\nlabels = counts_train.index\nvalues_train = counts_train.values\n\ndata = go.Pie(labels=labels, values=values_train ,pull=[0.03, 0])\nlayout = go.Layout(title='Comparing Tweet is a real disaster (1) or not (0) in %')\n\nfig = go.Figure(data=[data], layout=layout)\nfig.update_traces(hole=.3, hoverinfo=\"label+percent+value\")\nfig.update_layout(\n    # Add annotations in the center of the donut pies.\n    annotations=[dict(text='Train', x=0.5, y=0.5, font_size=20, showarrow=False)])\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['length'] = train['text'].apply(len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"data = [\n    go.Box(\n        y=train[train['target']==0]['length'],\n        name='Fake'\n    ),\n    go.Box(\n        y=train[train['target']==1]['length'],\n        name='Real'\n    )\n]\nlayout = go.Layout(\n    title = 'Comparison of text length in Tweets '\n)\nfig = go.Figure(data=data, layout=layout)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print('The max len of text',len(train.text.max()))\nprint('The min len of text is',len(train.text.min()))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.keyword.nunique()  # Total of 221 unique keywords","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntrain.keyword.value_counts()[:20].iplot(kind='bar', title='Top 20 keywords in text', color='red')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.location.value_counts()[:20].iplot(kind='bar', title='Top 20 location in tweet', color='blue')  # Check the top 15 locations ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" <a id='word'></a>\n#  4. WordCloud"},{"metadata":{"trusted":true},"cell_type":"code","source":"STOPWORDS.add('https')  # remove htps to the world Cloud\n\ndef Plot_world(text):\n    \n    comment_words = ' '\n    stopwords = set(STOPWORDS) \n    \n    for val in text: \n\n        # typecaste each val to string \n        val = str(val) \n\n        # split the value \n        tokens = val.split() \n\n        # Converts each token into lowercase \n        for i in range(len(tokens)): \n            tokens[i] = tokens[i].lower() \n\n        for words in tokens: \n            comment_words = comment_words + words + ' '\n\n\n    wordcloud = WordCloud(width = 5000, height = 4000, \n                    background_color ='black', \n                    stopwords = stopwords, \n                    min_font_size = 10).generate(comment_words) \n\n    # plot the WordCloud image                        \n    plt.figure(figsize = (12, 12), facecolor = 'k', edgecolor = 'k' ) \n    plt.imshow(wordcloud) \n    plt.axis(\"off\") \n    plt.tight_layout(pad = 0) \n\n    plt.show() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text = train.text.values\n\nPlot_world(text)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='clean'></a>\n# 5. Cleaning the text"},{"metadata":{"trusted":true},"cell_type":"code","source":"#How many http words has this text?\ntrain.loc[train['text'].str.contains('http')].target.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pattern = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n\ndef remove_html(text):\n    no_html= pattern.sub('',text)\n    return no_html","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove all text that start with html\ntrain['text']=train['text'].apply(lambda x : remove_html(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets check if this clean works\ntrain.loc[train['text'].str.contains('http')].target.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove all text that start with html in test\ntest['text']=test['text'].apply(lambda x : remove_html(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Now remove stopwords, pass to lower add delimiter and more"},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_text(text):\n \n    text = re.sub('[^a-zA-Z]', ' ', text)  \n\n    text = text.lower()  \n\n    # split to array(default delimiter is \" \") \n    text = text.split()  \n    \n    text = [w for w in text if not w in set(stopwords.words('english'))] \n\n    text = ' '.join(text)    \n            \n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text = train.text[3]\nprint(text)\nclean_text(text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Apply clean text \ntrain['text'] = train['text'].apply(lambda x : clean_text(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Apply clean text \ntest['text']=test['text'].apply(lambda x : clean_text(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# How many unique words have this text\ndef counter_word (text):\n    count = Counter()\n    for i in text.values:\n        for word in i.split():\n            count[word] += 1\n    return count","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_values = train[\"text\"]\n\ncounter = counter_word(text_values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"The len of unique words is: {len(counter)}\")\nlist(counter.items())[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='split'></a>\n# 6. Train Test Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"# The maximum number of words to be used. (most frequent)\n\nvocab_size = len(counter)\nembedding_dim = 32\n\n# Max number of words in each complaint.\nmax_length = 20\ntrunc_type='post'\npadding_type='post'\n\n# oov_took its set for words out our word index\noov_tok = \"<XXX>\"\ntraining_size = 6090\nseq_len = 12","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# this is base in 80% of the data, an only text and targert at this moment\n\ntraining_sentences = train.text[0:training_size]\ntraining_labels = train.target[0:training_size]\n\ntesting_sentences = train.text[training_size:]\ntesting_labels = train.target[training_size:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nprint('The Shape of training ',training_sentences.shape)\nprint('The Shape of testing',testing_sentences.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\ntokenizer.fit_on_texts(training_sentences)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_index = tokenizer.word_index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets see the first 10 elements\nprint(\"THe first word Index are: \")\nfor x in list(word_index)[0:15]:\n    print (\" {},  {} \".format(x,  word_index[x]))\n\n# If you want to see completed -> word_index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_sequences = tokenizer.texts_to_sequences(training_sentences)\ntraining_padded = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.text[1])\nprint(training_sequences[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## check Inverse for see how it works"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets see the first 10 elements\nprint(\"THe first reverse word Index are: \")\nfor x in list(reverse_word_index)[0:15]:\n    print (\" {},  {} \".format(x,  reverse_word_index[x]))\n\n# If you want to see completed -> reverse_word_index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def decode(text):\n    return ' '.join([reverse_word_index.get(i, '?') for i in text])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"decode(training_sequences[1]) # this can be usefull for check predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_padded[1628]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntesting_sequences = tokenizer.texts_to_sequences(testing_sentences)\ntesting_padded = pad_sequences(testing_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='model'></a>\n# 7. Creating the Model\n\n    # For a binary classification problem\n    model.compile(optimizer='adam',\n                  loss='binary_crossentropy',\n                  metrics=['accuracy'])\n                                    "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model Definition with LSTM\n\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n    tf.keras.layers.Dense(14, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')  # remember this is a binary clasification\n])\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.utils import plot_model\nplot_model(model, to_file='model_plot4a.png', show_shapes=True, show_layer_names=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nstart_time = time.time()\n\nnum_epochs = 10\nhistory = model.fit(training_padded, training_labels, epochs=num_epochs, validation_data=(testing_padded, testing_labels))\n\nfinal_time = (time.time()- start_time)/60\nprint(f'The time in minutos: {final_time}')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_loss = pd.DataFrame(model.history.history)\nmodel_loss.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_loss[['accuracy','val_accuracy']].plot(ylim=[0,1]);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='eval'></a>\n# 8. Model Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = model.predict_classes(testing_padded)   # predict_ clases because is classification problem with the split test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report,confusion_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Showing Confusion Matrix\ndef plot_cm(y_true, y_pred, title, figsize=(5,4)):\n    cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n    cm_sum = np.sum(cm, axis=1, keepdims=True)\n    cm_perc = cm / cm_sum.astype(float) * 100\n    annot = np.empty_like(cm).astype(str)\n    nrows, ncols = cm.shape\n    for i in range(nrows):\n        for j in range(ncols):\n            c = cm[i, j]\n            p = cm_perc[i, j]\n            if i == j:\n                s = cm_sum[i]\n                annot[i, j] = '%.1f%%\\n%d/%d' % (p, c, s)\n            elif c == 0:\n                annot[i, j] = ''\n            else:\n                annot[i, j] = '%.1f%%\\n%d' % (p, c)\n    cm = pd.DataFrame(cm, index=np.unique(y_true), columns=np.unique(y_true))\n    cm.index.name = 'Actual'\n    cm.columns.name = 'Predicted'\n    fig, ax = plt.subplots(figsize=figsize)\n    plt.title(title)\n    sns.heatmap(cm, cmap= \"YlGnBu\", annot=annot, fmt='', ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Showing Confusion Matrix\nplot_cm(testing_labels,predictions, 'Confution matrix of Tweets', figsize=(7,7))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Now working with test dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntesting_sequences2 = tokenizer.texts_to_sequences(test.text)\ntesting_padded2 = pad_sequences(testing_sequences2, maxlen=max_length, padding=padding_type, truncating=trunc_type)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = model.predict(testing_padded2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sample of submission\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission['target'] = (predictions > 0.5).astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv(\"submission.csv\", index=False, header=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>I hope this notebook <span style=\"color:red\">Usefull</span> for you! </h3>"},{"metadata":{},"cell_type":"markdown","source":"<a href=\"#top\" class=\"btn btn-primary btn-lg active\" role=\"button\" aria-pressed=\"true\">Go to TOP</a>\n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}